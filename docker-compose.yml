services:
  t-1985:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: t-1985
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf -c 0  --port 8080

  dev-assist:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: dev-assist
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf -c 0 --port 8081

  dev-assist-knowledge:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: dev-assist-knowledge
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/nomic-embed-text-v1.5.Q4_K_M.gguf -c 0 --port 8082 --embeddings

  skynet:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/skynet.dockerfile
    container_name: skynet
    volumes:
      - ./package:/data/package
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    entrypoint: skynet_entrypoint.sh

  voidgen:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/voidgen.dockerfile
    container_name: voidgen
    runtime: nvidia
    ports:
      - "8002:8002"
    volumes:
      - ./voidgen_data/input:/app/ComfyUI/input
      - ./voidgen_data/user:/app/ComfyUI/user

    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  link-85:
    container_name: link-85
    image: ghcr.io/open-webui/open-webui:main
    environment:
      - ENABLE_OLLAMA_API=false
    ports:
      - "8001:8080"
    volumes:
      - ./link-85_data:/app/backend/data
    depends_on:
        - continue-proxy

  continue-proxy:
    container_name: continue-proxy
    image: nginx:latest
    ports:
      - "8005:8005"
    volumes:
      - ./configs/nginx.conf:/etc/nginx/nginx.conf:ro
