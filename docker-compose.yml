services:
  t-1985:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: t-1985
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - T1985_MODEL_NAME=${T1985_model_name}
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/${T1985_MODEL_NAME} -c 0  --port 8080
    depends_on:
      - link-85

  dev-assist:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: dev-assist
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf -c 0 --port 8081
    depends_on:
      - link-85

  dev-assist-autocomplete:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: dev-assist-autocomplete
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/AceCoder-Qwen2.5-Coder-7B-Base-Rule.Q4_K_M.gguf -c 0 --port 8082
    depends_on:
      - link-85

  skynet:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/skynet.dockerfile
    container_name: skynet
    volumes:
      - ./package:/data/package
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    entrypoint: skynet_entrypoint.sh

  voidgen:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/voidgen.dockerfile
    container_name: voidgen
    runtime: nvidia
    ports:
      - "8002:8002"
    volumes:
      - ./voidgen_data/input:/app/ComfyUI/input
      - ./voidgen_data/user:/app/ComfyUI/user

    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  link-85:
    container_name: link-85
    image: ghcr.io/open-webui/open-webui:main
    environment:
      - ENABLE_OLLAMA_API=false
    ports:
      - "8001:8080"
    volumes:
      - ./link-85_data:/app/backend/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 5s
      retries: 5
      start_period: 5s
      timeout: 2s

  continue-proxy:
    container_name: continue-proxy
    image: nginx:latest
    ports:
      - "8005:8005"
    volumes:
      - ./configs/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      link-85:
        condition: service_healthy
