services:
  t-1985:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/t1985.dockerfile
    container_name: t-1985
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/opt/llama-t1985/models:ro
    ports:
      - 8000:8080
    entrypoint: "llama-server"
    command: --host 0.0.0.0 --n-gpu-layers 50 -m /opt/llama-t1985/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf -c 0

  skynet:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/skynet.dockerfile
    container_name: skynet
    volumes:
      - ./package:/data/package
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    entrypoint: skynet_entrypoint.sh

  voidgen:
    build:
      args:
        GROUP_ID: ${GROUP_ID:-1000}
        USER_ID: ${USER_ID:-1000}
      context: .
      dockerfile: docker/voidgen.dockerfile
    container_name: voidgen
    runtime: nvidia
    ports:
      - "8002:8188"
    volumes:
      - ./models:/app/ComfyUI/models:ro
      - ./loras:/app/ComfyUI/loras:ro
      - ./output:/app/ComfyUI/output
      - ./voidgen_data:/app/comfyUI/user
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  link-85:
      image: ghcr.io/open-webui/open-webui:main
      environment:
        - OPENAI_API_BASE_URL=http://t-1985:8080/v1
        - OPENAI_API_KEY=local
        - ENABLE_OLLAMA_API=false
      ports:
        - "8001:8080"
      depends_on:
        - t-1985
      volumes:
        - ./link-85_data:/app/backend/data
